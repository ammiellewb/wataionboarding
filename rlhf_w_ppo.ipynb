{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MSQwGOYJohW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install flake8 datasets transformers -U trl torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !pip install --upgrade \"safetensors>=0.4.3\"\n",
    "# !{sys.executable} -m pip install --upgrade --force-reinstall \"safetensors>=0.4.3\"\n",
    "# !{sys.executable} -m pip install --upgrade trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMaJ02mXPIS-"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer as ClsTrainer,\n",
    "    TrainingArguments as ClsTrainingArguments,\n",
    ")\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKJSpNh8oI1u"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aV6In-ntJufA"
   },
   "outputs": [],
   "source": [
    "# stream data\n",
    "stream_ds = load_dataset(\n",
    "    \"codeparrot/codeparrot-clean\",\n",
    "    split=\"train[:0.1%]\",\n",
    "    token=True\n",
    ")\n",
    "\n",
    "# shuffle a bit then take the first 500 examples\n",
    "import random\n",
    "random.seed(42)\n",
    "shuffled = stream_ds.shuffle(buffer_size=10_000)  # small in-memory buffer\n",
    "small_iter = itertools.islice(shuffled, 2000)\n",
    "\n",
    "# print top 5 code lines\n",
    "for idx, ex in enumerate(small_iter):\n",
    "    print(idx, ex[\"content\"][:50])\n",
    "    if idx >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_DEERjBNjRP"
   },
   "outputs": [],
   "source": [
    "# compute a style score from 0.0 to 1.0 using flake8\n",
    "def get_style_score(code: str, max_vios: int = 10) -> float:\n",
    "  with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as tf:\n",
    "    tf.write(code.encode(\"utf-8\"))\n",
    "    tf.flush()\n",
    "    path = tf.name\n",
    "  result = subprocess.run(\n",
    "      [\"flake8\", \"--max-line-length=88\", path],\n",
    "      capture_output=True,\n",
    "      text=True\n",
    "  )\n",
    "  vios = len(result.stdout.splitlines())\n",
    "  Path(path).unlink()\n",
    "\n",
    "  return max(0.0, 1.0-vios/max_vios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGp1gHNkQVdU"
   },
   "outputs": [],
   "source": [
    "# collect compliant snippts for fine-tuning\n",
    "compliant_snippets = []\n",
    "\n",
    "for ex in small_iter:\n",
    "  code = ex[\"content\"]\n",
    "  # print(get_style_score(code))\n",
    "  if get_style_score(code) == 1.0:\n",
    "    compliant_snippets.append(code)\n",
    "  if len(compliant_snippets) >= 200:\n",
    "    break\n",
    "\n",
    "# compliant_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCleEkzHSInz"
   },
   "outputs": [],
   "source": [
    "# collect 200 mixed snippets for labelled reward model\n",
    "labeled_data = []\n",
    "for ex in small_iter:\n",
    "  code = ex[\"content\"]\n",
    "  label = int(get_style_score(code) == 1.0)\n",
    "  labeled_data.append({\"code\": code, \"label\": label})\n",
    "  if len(labeled_data) >= 400:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APfSQOBBS3A2"
   },
   "outputs": [],
   "source": [
    "# fine-tune CodeParrot on compliant snippets\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer.save_pretrained(\"codeparrot-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEkgsaOqWeb8"
   },
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "encodings = tokenizer(\n",
    "    compliant_snippets,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWq9xCmeV7-i"
   },
   "outputs": [],
   "source": [
    "class LMData(torch.utils.data.Dataset):\n",
    "  def __init__(self, enc):\n",
    "    self.input_ids = enc.input_ids\n",
    "    self.attn_mask = enc.attention_mask\n",
    "  def __len__(self): return len(self.input_ids)\n",
    "  def __getitem__(self, idx): return {\n",
    "      \"input_ids\": self.input_ids[idx],\n",
    "      \"attention_mask\": self.attn_mask[idx],\n",
    "      \"labels\": self.input_ids[idx]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZAX6REe1U9H"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO1Am5RQV-ah"
   },
   "outputs": [],
   "source": [
    "lm_dataset = LMData(encodings)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "lm_args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ft\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1\n",
    ")\n",
    "lm_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=lm_args,\n",
    "    train_dataset=lm_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "lm_trainer.train()\n",
    "model.save_pretrained(\"codeparrot-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YT21k4tAZiPE"
   },
   "outputs": [],
   "source": [
    "# reward model: small classifier on style adherence\n",
    "cls_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "cls_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# prepare classification dataset\n",
    "texts = [d[\"code\"] for d in labeled_data]\n",
    "labels = [d[\"label\"] for d in labeled_data]\n",
    "cls_enc = cls_tokenizer(texts, truncation=True, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NwOb0pdZ8OP"
   },
   "outputs": [],
   "source": [
    "class CLSData(torch.utils.data.Dataset):\n",
    "  def __init__(self, enc, labels):\n",
    "    self.input_ids = enc.input_ids\n",
    "    self.attn_mask = enc.attention_mask\n",
    "    self.labels = torch.tensor(labels)\n",
    "  def __len__(self): return len(self.labels)\n",
    "  def __getitem__(self, idx): return {\"input_ids\": self.input_ids[idx], \"attention_mask\": self.attn_mask[idx], \"labels\": self.labels[idx]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEjPLt3DahLR"
   },
   "outputs": [],
   "source": [
    "cls_dataset = CLSData(cls_enc, labels)\n",
    "cls_args = ClsTrainingArguments(output_dir=\"style-cls\", per_device_train_batch_size=8, num_train_epochs=1, logging_steps=10, save_total_limit=1)\n",
    "cls_trainer = ClsTrainer(model=cls_model, args=cls_args, train_dataset=cls_dataset)\n",
    "\n",
    "cls_trainer.train()\n",
    "cls_model.save_pretrained(\"style-cls\")\n",
    "\n",
    "# reward model (style classifier)\n",
    "reward_model = cls_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mSGexhXtxUD"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# prepare prompts for PPO\n",
    "test_prompts = [\"def add(a, b):\", \"class Person:\", \"def compute():\", \"def process_data(data):\"]\n",
    "raw_dataset = Dataset.from_dict({\"query\": test_prompts})\n",
    "\n",
    "def tokenize_prompts(ex):\n",
    "  output = tokenizer(ex[\"query\"], truncation=True, padding=\"max_length\", max_length=32)\n",
    "  output[\"input_ids\"] = output[\"input_ids\"]\n",
    "  output[\"attention_mask\"] = output[\"attention_mask\"]\n",
    "  return output\n",
    "\n",
    "train_dataset = raw_dataset.map(tokenize_prompts, batched=True, remove_columns=[\"query\"])\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM_SnwORuHd-"
   },
   "outputs": [],
   "source": [
    "# fine-tuned tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codeparrot-ft\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItI2fX7Det53"
   },
   "outputs": [],
   "source": [
    "from trl import create_reference_model\n",
    "\n",
    "# RLHF via PPO: 2 gradient updates\n",
    "ppo_config = PPOConfig(\n",
    "    output_dir=\"results/style-ppo\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.41e-5,\n",
    "\n",
    "    # PPO-specific\n",
    "    sft_model_path=\"codeparrot-ft\",\n",
    "    reward_model_path=\"style-cls\",\n",
    "    exp_name=\"style-ppo\",\n",
    "    batch_size=4,\n",
    "    mini_batch_size=4,\n",
    "    num_ppo_epochs=1,\n",
    "    total_episodes=2 # 2 generate->update loops\n",
    ")\n",
    "\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"codeparrot-ft\")\n",
    "\n",
    "# models return dicts rather than tuples\n",
    "ppo_model.config.return_dict = True\n",
    "ppo_model.pretrained_model.config.return_dict = True\n",
    "\n",
    "# reference copy\n",
    "ref_model = create_reference_model(ppo_model)\n",
    "# ref_model.eval()  # no updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "welz6phmh9FB"
   },
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# attach a GenerationConfig\n",
    "gen_conf = GenerationConfig(**ppo_model.config.to_dict())\n",
    "ppo_model.generation_config = gen_conf\n",
    "ref_model.generation_config = gen_conf\n",
    "\n",
    "# attach base_model_prefix, matches config.model_type\n",
    "ppo_model.base_model_prefix = \"pretrained_model\"\n",
    "ref_model.base_model_prefix = \"pretrained_model\"\n",
    "\n",
    "ppo_model.to(device)\n",
    "ref_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcdH907uCXPO"
   },
   "outputs": [],
   "source": [
    "def ensure_dict_output(model):\n",
    "    \"\"\"Wrapper to ensure model always returns dict format\"\"\"\n",
    "    original_forward = model.forward\n",
    "\n",
    "    def wrapped_forward(*args, **kwargs):\n",
    "        # Force return_dict=True in the forward call\n",
    "        kwargs['return_dict'] = True\n",
    "        output = original_forward(*args, **kwargs)\n",
    "        if isinstance(output, tuple):\n",
    "            # Convert tuple to dict format as fallback\n",
    "            from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "            return CausalLMOutputWithPast(\n",
    "                logits=output[0],\n",
    "                past_key_values=output[1] if len(output) > 1 else None,\n",
    "                hidden_states=output[2] if len(output) > 2 else None,\n",
    "                attentions=output[3] if len(output) > 3 else None,\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    model.forward = wrapped_forward\n",
    "    return model\n",
    "def fix_pretrained_model_forward(model):\n",
    "    \"\"\"Specifically fix the pretrained_model forward method\"\"\"\n",
    "    if hasattr(model, 'pretrained_model'):\n",
    "        original_forward = model.pretrained_model.forward\n",
    "\n",
    "        def wrapped_forward(*args, **kwargs):\n",
    "            kwargs['return_dict'] = True\n",
    "            return original_forward(*args, **kwargs)\n",
    "\n",
    "        model.pretrained_model.forward = wrapped_forward\n",
    "    return model\n",
    "\n",
    "# Apply the fix to both models:\n",
    "ref_model = fix_pretrained_model_forward(ref_model)\n",
    "ppo_model = fix_pretrained_model_forward(ppo_model)\n",
    "\n",
    "# Also apply the general wrapper as backup\n",
    "ref_model = ensure_dict_output(ref_model)\n",
    "ppo_model = ensure_dict_output(ppo_model)\n",
    "\n",
    "ppo_model.to(device)\n",
    "ref_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5AkWCfclmfw"
   },
   "outputs": [],
   "source": [
    "# defining reward function using reward model\n",
    "def reward_fn(responses):\n",
    "  texts = [tokenizer.decode(r, skip_special_tokens=True) for r in responses]\n",
    "  return [torch.tensor(get_style_score(t), device=device) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPv-cVArmhbf"
   },
   "outputs": [],
   "source": [
    "# before RLHF: baseline generation\n",
    "test_prompts = [\"def add(a, b):\", \"class Person:\", \"def compute():\", \"def process_data(data):\"]\n",
    "baseline_scores = []\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "for q in test_prompts:\n",
    "    enc = tokenizer(q, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    out = ppo_model.generate(**enc, max_length=50, pad_token_id=tokenizer.pad_token_id)[0]\n",
    "    baseline_scores.append(get_style_score(tokenizer.decode(out, skip_special_tokens=True)))\n",
    "\n",
    "print(baseline_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gzyvj3bWCJ7m"
   },
   "outputs": [],
   "source": [
    "print(\"PPO model return_dict:\", ppo_model.config.return_dict)\n",
    "print(\"PPO pretrained_model return_dict:\", ppo_model.pretrained_model.config.return_dict)\n",
    "print(\"Ref model return_dict:\", ref_model.config.return_dict)\n",
    "print(\"Ref pretrained_model return_dict:\", ref_model.pretrained_model.config.return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taLZhWlnCZ5T"
   },
   "outputs": [],
   "source": [
    "def compute_reward_scores(query_tensors, response_tensors):\n",
    "    \"\"\"Compute reward scores for PPO training\"\"\"\n",
    "    rewards = []\n",
    "    for query, response in zip(query_tensors, response_tensors):\n",
    "        # Combine query and response\n",
    "        full_sequence = torch.cat([query, response])\n",
    "        # Decode to text\n",
    "        text = tokenizer.decode(full_sequence, skip_special_tokens=True)\n",
    "        # Get style score\n",
    "        score = get_style_score(text)\n",
    "        rewards.append(torch.tensor(score, device=device))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNN_WuqhEm16"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.signature(PPOTrainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxfAxTNotp6K"
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate PPOTrainer with required args\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    processing_class=tokenizer,\n",
    "    model=ppo_model,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model,\n",
    "    train_dataset=train_dataset,\n",
    "    value_model=ppo_model,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmaAKpa03s0P"
   },
   "outputs": [],
   "source": [
    "# run 2 PPO gradient updates\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msNZq13Tm5Jm"
   },
   "outputs": [],
   "source": [
    "# after RLHF: post-PPO generation\n",
    "post_scores = []\n",
    "\n",
    "for q in test_prompts:\n",
    "    enc = tokenizer(q, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    out = ppo_model.generate(**enc, max_length=50, pad_token_id=tokenizer.pad_token_id)[0]\n",
    "    post_scores.append(get_style_score(tokenizer.decode(out, skip_special_tokens=True)))\n",
    "\n",
    "print(post_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pu-7evvzoYvV"
   },
   "outputs": [],
   "source": [
    "# eval: comparing style scores\n",
    "for i, q in enumerate(test_prompts):\n",
    "  print(f\"Prompt: {q}\")\n",
    "  print(f\"Baseline score: {baseline_scores[i]}\")\n",
    "  print(f\"Post-RLHF score: {post_scores[i]})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
